{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "298035d4-5673-4977-8393-262f92cc7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "ff3c8dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_seq_len = 45\n",
    "input_dim = 512\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "ffn_hidden = 2048\n",
    "num_layers = 6\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b2d3ac",
   "metadata": {},
   "source": [
    "## Multihead Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "deac3059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q,k,v,mask=None):\n",
    "    d_k = q.size(-1)\n",
    "    qk = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "    if mask is not None and mask is True:\n",
    "        mask = torch.full(qk.size(),fill_value= float('-inf'))\n",
    "        mask = torch.triu(mask, diagonal=1)\n",
    "        qk = qk + mask \n",
    "    qk = F.softmax(qk, dim=-1)\n",
    "    new_qkv = torch.matmul(qk, v)\n",
    "    return new_qkv\n",
    "\n",
    "class Multihead_Self_Attention(nn.Module):\n",
    "    def __init__(self,input_dim, d_model, num_heads):\n",
    "        super(Multihead_Self_Attention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.model_dim = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = self.model_dim // self.num_heads\n",
    "        self.qkv_layer = nn.Linear(input_dim, 3 * self.model_dim)\n",
    "        self.concat_layer = nn.Linear(self.model_dim, self.model_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self,x,mask=None):\n",
    "        qkv = self.qkv_layer(x)\n",
    "        qkv = qkv.view(batch_size,max_seq_len,self.num_heads,3*self.head_dim)\n",
    "        qkv = qkv.permute(0,2,1,3)\n",
    "        q,k,v = qkv.chunk(3,dim=-1)\n",
    "        new_qkv = scaled_dot_product_attention(q,k,v,mask)\n",
    "        new_qkv = new_qkv.view(batch_size,max_seq_len,self.model_dim)\n",
    "        out = self.concat_layer(new_qkv)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a55ca48",
   "metadata": {},
   "source": [
    "## Multihead Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "7ee818ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multihead_Cross_Attention(nn.Module):\n",
    "    def __init__(self,input_dim, model_dim, num_heads):\n",
    "        super(Multihead_Cross_Attention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.model_dim = model_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = model_dim // num_heads\n",
    "        self.qk_layer = nn.Linear(input_dim, 2 * model_dim)\n",
    "        self.v_layer = nn.Linear(input_dim, model_dim)\n",
    "        self.concat_layer = nn.Linear(model_dim, model_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self,x,y,mask=None):\n",
    "        qk = self.qk_layer(x)\n",
    "        v = self.v_layer(y)\n",
    "        qk = qk.view(batch_size,max_seq_len,num_heads,2*self.head_dim)\n",
    "        v = v.view(batch_size,max_seq_len,num_heads,self.head_dim)\n",
    "        qk = qk.permute(0,2,1,3)\n",
    "        v = v.permute(0,2,1,3)\n",
    "        q,k = qk.chunk(2,dim=-1)\n",
    "        new_qkv = scaled_dot_product_attention(q,k,v,mask)\n",
    "        new_qkv = new_qkv.view(batch_size,max_seq_len,self.model_dim)\n",
    "        out = self.concat_layer(new_qkv)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f160b031",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "5f0c6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostionalEncoding(nn.Module):\n",
    "    def __init__(self,max_seq_len,d_model):\n",
    "        super(PostionalEncoding,self).__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.d_model = d_model\n",
    "        self.encoding = torch.zeros(self.max_seq_len,self.d_model)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        even_index = torch.arange(0,self.d_model,2).float()\n",
    "        domenator = torch.pow(10000,even_index/self.d_model)\n",
    "        position = torch.arange(0,self.max_seq_len).unsqueeze(1)\n",
    "        PE_even = torch.sin(position/domenator)\n",
    "        PE_odd = torch.cos(position/domenator)\n",
    "        stacked = torch.stack([PE_even,PE_odd],dim=2)\n",
    "        PE_flatten = torch.flatten(stacked,start_dim=1,end_dim=2)\n",
    "        return PE_flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfde7de",
   "metadata": {},
   "source": [
    "## Normalization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "9ed90324",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizationLayer(nn.Module):\n",
    "    def __init__(self, parameter_dim):\n",
    "        super(NormalizationLayer, self).__init__()\n",
    "        self.parameters_shape = parameter_dim\n",
    "        self.gamma = nn.Parameter(torch.ones(parameter_dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(parameter_dim))\n",
    "        self.eps = 1e-6\n",
    "\n",
    "    def forward(self, x):\n",
    "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
    "        mean = x.mean(dims, keepdim=True)\n",
    "        std = x.std(dims, keepdim=True)\n",
    "        out = self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47adc7b1",
   "metadata": {},
   "source": [
    "## Position-wise Feed-Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "31a2a97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,d_model,ffn_hidden):\n",
    "        super(FeedForward,self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.ffn_hidden = ffn_hidden\n",
    "        self.layer1 = nn.Linear(self.d_model,self.ffn_hidden)\n",
    "        self.layer2 = nn.Linear(self.ffn_hidden,self.d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1bf3a1",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "04876c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Layer(nn.Module):\n",
    "    def __init__(self,num_heads, d_model, ffn_hidden, max_seq_len, dropout_rate):\n",
    "        super(Encoder_Layer,self).__init__()\n",
    "        self.multihead_attention = Multihead_Self_Attention(input_dim,d_model,num_heads)\n",
    "        self.pos_encoding = PostionalEncoding(max_seq_len,d_model)\n",
    "        self.feedforward = FeedForward(d_model,ffn_hidden)\n",
    "        self.norm1 = NormalizationLayer([d_model])\n",
    "        self.norm2 = NormalizationLayer([d_model])\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        reseduial_x = x\n",
    "        x = self.multihead_attention(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = x + reseduial_x\n",
    "        x = self.norm1(x)\n",
    "        reseduial_x = x\n",
    "        x = self.feedforward(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = x + reseduial_x\n",
    "        x = self.norm2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "5ac0aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ecoder(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, dropout_rate, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(*[Encoder_Layer(num_heads, d_model, ffn_hidden, max_seq_len, dropout_rate)\n",
    "                                     for _ in range(num_layers)])\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c241c7b",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "8d18f409",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_Layer(nn.Module):\n",
    "    def __init__(self,num_heads, d_model, ffn_hidden, max_seq_len, dropout_rate):\n",
    "        super(Decoder_Layer,self).__init__()\n",
    "        self.multihead_self_attention = Multihead_Self_Attention(input_dim,d_model,num_heads)\n",
    "        self.multihead_cross_attention = Multihead_Cross_Attention(input_dim,d_model,num_heads)\n",
    "        self.feedforward = FeedForward(d_model,ffn_hidden)\n",
    "        self.norm1 = NormalizationLayer([d_model])\n",
    "        self.norm2 = NormalizationLayer([d_model])\n",
    "        self.norm3 = NormalizationLayer([d_model])\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        reseduial_y = y\n",
    "        y = self.multihead_self_attention(y,mask = True)\n",
    "        y = self.dropout1(y)\n",
    "        y = y + reseduial_y\n",
    "        y = self.norm1(y)\n",
    "\n",
    "        reseduial_y = y\n",
    "        y = self.multihead_cross_attention(x,y)\n",
    "        y = self.dropout2(y)\n",
    "        y = y + reseduial_y\n",
    "        y = self.norm2(y)\n",
    "\n",
    "        reseduial_y = y\n",
    "        y = self.feedforward(y)\n",
    "        y = self.dropout3(y)\n",
    "        y = y + reseduial_y\n",
    "        y = self.norm3(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "dfafde66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential_Decoder(nn.Sequential):\n",
    "    def forward(self,*input):\n",
    "        x, y = input\n",
    "        for module in self._modules.values():\n",
    "            y = module(x, y) \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "8612abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, dropout_rate, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = Sequential_Decoder(*[Decoder_Layer(num_heads, d_model, ffn_hidden, max_seq_len, dropout_rate)\n",
    "                                     for _ in range(num_layers)])\n",
    "    def forward(self, x, y):\n",
    "        y = self.layers(x, y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46fd7d3",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "fad23c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = '<START>'\n",
    "PADDING_TOKEN = '<PADDING>'\n",
    "END_TOKEN = '<END>' \n",
    "\n",
    "## all characters in the arabic language + symbols\n",
    "arabic_voc = [START_TOKEN,'ا','ب','ت','ث','ج','ح','خ','د','ذ','ر','ز','س','ش','ص','ض','ط','ظ','ع','غ','ف','ق','ك','ل','م','ن','ه','و','ي','ء','آ','أ','ؤ','إ','ئ','ئ','ة','ـ','،','؛','؟','٠','١','٢','٣','٤','٥','٦','٧','٨','٩','٪','٫','٬','٭','ٮ','ٯ','ٰ','ٱ','ٲ','ٳ','ٴ','ٵ','ٶ','ٷ','ٸ','ٹ','ٺ','ٻ','ټ','ٽ','پ','ٿ','ڀ','ځ','ڂ','ڃ','ڄ','څ','چ','ڇ','ڈ','ډ','ڊ','ڋ','ڌ','ڍ','ڎ','ڏ','ڐ','ڑ','ڒ','ړ','ڔ','ڕ','ږ','ڗ','ژ','ڙ','ښ','ڛ','ڜ','ڝ','ڞ','ڟ','ڠ','ڡ','ڢ','ڣ','ڤ','ڥ','ڦ','ڧ','ڨ','ک','ڪ','ګ','ڬ','ڭ','ڮ','گ','ڰ','ڱ','ڲ','ڳ','ڴ','ڵ','ڶ','ڷ','ڸ','ڹ','ں','ڻ','ڼ','ڽ','ھ','ڿ','ۀ','ہ','ۂ','ۃ','ۄ','ۅ','ۆ','ۇ','ۈ','ۉ','ۊ','ۋ','ی','ۍ','ێ','ۏ','ې','ۑ','ے','ۓ','۔','ە','ۖ','ۗ','ۘ','ۙ','ۚ','ۛ','ۜ','۝','۞','۟','۠','ۡ','ۢ','ۣ','ۤ','ۥ','ۦ','ۧ',' ','!','\"','#','$','%','&',\"'\",'(',')','*','+',',','-','.','/',':',';','<','=','>','?','@','[','\\\\',']','^','_','`','{','|','}','~',' ','\\t','\\n','\\r','\\x0b','\\x0c',PADDING_TOKEN,END_TOKEN]\n",
    "## all characters in the english language + symbols\n",
    "english_voc = [START_TOKEN, 'A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','0','1','2','3','4','5','6','7','8','9','!','\"','#','$','%','&',\"'\",'(',')','*','+',',','-','.','/',':',';','<','=','>','?','@','[','\\\\',']','^','_','`','{','|','}','~',' ','\\t','\\n','\\r','\\x0b','\\x0c',PADDING_TOKEN,END_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "c46f378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_english = {k:v for k,v in enumerate(english_voc)}\n",
    "english_to_index = {v:k for k,v in enumerate(english_voc)}\n",
    "index_to_arabic = {k:v for k,v in enumerate(arabic_voc)}\n",
    "arabic_to_index = {v:k for k,v in enumerate(arabic_voc)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "218bbe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEmbedding(nn.Module):\n",
    "    def __init__(self, language_to_index, max_seq_len, d_model):\n",
    "        super(SentenceEmbedding,self).__init__()\n",
    "        self.vocab_size = len(language_to_index)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.language_to_index = language_to_index\n",
    "        self.position_encoder = PostionalEncoding(max_seq_len,d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.START_TOKEN = START_TOKEN\n",
    "        self.END_TOKEN = END_TOKEN\n",
    "        self.PADDING_TOKEN = PADDING_TOKEN\n",
    "       \n",
    "    def batch_tokenize(self, batch, start_token=True, end_token=True):\n",
    "\n",
    "        def tokenize(sentence, start_token=True, end_token=True):\n",
    "            sentence_word_indicies = [self.language_to_index[token] for token in list(sentence)]\n",
    "            if start_token:\n",
    "                sentence_word_indicies.insert(0, self.language_to_index[self.START_TOKEN])\n",
    "            if end_token:\n",
    "                sentence_word_indicies.append(self.language_to_index[self.END_TOKEN])\n",
    "            for _ in range(len(sentence_word_indicies), self.max_seq_len):\n",
    "                sentence_word_indicies.append(self.language_to_index[self.PADDING_TOKEN])\n",
    "            return torch.tensor(sentence_word_indicies)\n",
    "\n",
    "        tokenized = []\n",
    "        for sentence_num in range(len(batch)):\n",
    "           tokenized.append( tokenize(batch[sentence_num], start_token, end_token) )\n",
    "        \n",
    "        tokenized = torch.stack(tokenized)\n",
    "        return tokenized\n",
    "    \n",
    "    def forward(self, x, start_token= True, end_token=True): \n",
    "        x = self.batch_tokenize(x ,start_token=start_token, end_token=end_token)\n",
    "        x = self.embedding(x)\n",
    "        pos = self.position_encoder()\n",
    "        x = self.dropout(x + pos)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "de915ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_INFTY = -1e9\n",
    "\n",
    "def create_masks(eng_batch, ar_batch):\n",
    "    num_sentences = len(eng_batch)\n",
    "    look_ahead_mask = torch.full([max_seq_len, max_seq_len] , True)\n",
    "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
    "    encoder_padding_mask = torch.full([num_sentences, max_seq_len, max_seq_len] , False)\n",
    "    decoder_padding_mask_self_attention = torch.full([num_sentences, max_seq_len, max_seq_len] , False)\n",
    "    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_seq_len, max_seq_len] , False)\n",
    "\n",
    "    for idx in range(num_sentences):\n",
    "      eng_sentence_length, ar_sentence_length = len(eng_batch[idx]), len(ar_batch[idx])\n",
    "      eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_seq_len)\n",
    "      ar_chars_to_padding_mask = np.arange(max_seq_len + 1, max_seq_len)\n",
    "      encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
    "      encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
    "      decoder_padding_mask_self_attention[idx, :, ar_chars_to_padding_mask] = True\n",
    "      decoder_padding_mask_self_attention[idx, ar_chars_to_padding_mask, :] = True\n",
    "      decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
    "      decoder_padding_mask_cross_attention[idx, ar_chars_to_padding_mask, :] = True\n",
    "\n",
    "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
    "    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
    "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
    "    \n",
    "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b70522",
   "metadata": {},
   "source": [
    "## Loading The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "0de81964",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/en_ar_final.tsv', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "ff99355c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1325899, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>ar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and this</td>\n",
       "      <td>و هذه؟</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it was um</td>\n",
       "      <td>...لقد كان</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what is she doing here</td>\n",
       "      <td>ما الذي تفعله هناك؟</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i dont like it</td>\n",
       "      <td>لا أحب ذلك</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>did you get the part</td>\n",
       "      <td>هل حصلت على جزء ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       en                   ar\n",
       "0                and this               و هذه؟\n",
       "1               it was um           ...لقد كان\n",
       "2  what is she doing here  ما الذي تفعله هناك؟\n",
       "3          i dont like it           لا أحب ذلك\n",
       "4    did you get the part    هل حصلت على جزء ?"
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "bd8c1794",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, english_sentences, arabic_sentences):\n",
    "        self.english_sentences = english_sentences\n",
    "        self.arabic_sentences = arabic_sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.english_sentences[idx], self.arabic_sentences[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02a901b",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "83054e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "# drop duplicates\n",
    "df = df.drop_duplicates()\n",
    "# drop rows with empty strings\n",
    "df = df[(df['en'] != '') & (df['ar'] != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "7ddf9a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences = df['en'].values\n",
    "arabic_sentences = df['ar'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "273b79e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and this', 'it was um', 'what is she doing here', ...,\n",
       "       'chinas quest for value', 'zimbabweu0027s last chance',\n",
       "       'zuma rising'], dtype=object)"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "3e6b9943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['و هذه؟', '...لقد كان', 'ما الذي تفعله هناك؟', ...,\n",
       "       'الصين تفتش عن القيمة', 'زيمبابوي والفرصة الأخيرة',\n",
       "       'صعود نجم زوما'], dtype=object)"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arabic_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "f02111e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and this و هذه؟\n"
     ]
    }
   ],
   "source": [
    "print(english_sentences[0],arabic_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "f2181bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 242)"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(x) for x in arabic_sentences), max(len(x) for x in english_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "1338f846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91th percentile length Arabic: 28.0\n",
      "91th percentile length English: 44.0\n"
     ]
    }
   ],
   "source": [
    "PERCENTILE = 91\n",
    "print( f\"{PERCENTILE}th percentile length Arabic: {np.percentile([len(x) for x in arabic_sentences], PERCENTILE)}\" )\n",
    "print( f\"{PERCENTILE}th percentile length English: {np.percentile([len(x) for x in english_sentences], PERCENTILE)}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "60f8bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top = 1000000\n",
    "# english_sentences = english_sentences[:top]\n",
    "# arabic_sentences = arabic_sentences[:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "ccaf0232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 831789\n",
      "Number of valid sentences: 547389\n"
     ]
    }
   ],
   "source": [
    "def is_valid_tokens(sentence, vocab):\n",
    "    for token in list(set(sentence)):\n",
    "        if token not in vocab:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_valid_length(sentence, max_seq_len):\n",
    "    return len(list(sentence)) < (max_seq_len - 1) \n",
    "\n",
    "valid_sentence_indicies = []\n",
    "for index in range(len(arabic_sentences)):\n",
    "    arabic_sentence, english_sentence = arabic_sentences[index], english_sentences[index]\n",
    "    if is_valid_length(arabic_sentence, max_seq_len) \\\n",
    "        and is_valid_length(english_sentence, max_seq_len) \\\n",
    "        and is_valid_tokens(arabic_sentence, arabic_voc) \\\n",
    "        and is_valid_tokens(english_sentence, english_voc) :\n",
    "        valid_sentence_indicies.append(index)\n",
    "\n",
    "print(f\"Number of sentences: {len(arabic_sentences)}\")\n",
    "print(f\"Number of valid sentences: {len(valid_sentence_indicies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "b698571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_sentences = [arabic_sentences[i] for i in valid_sentence_indicies]\n",
    "english_sentences = [english_sentences[i] for i in valid_sentence_indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "a365501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'en': english_sentences, 'ar': arabic_sentences})\n",
    "df.to_csv('dataset/updated_en_ar_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "f1e15bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDataset(english_sentences, arabic_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "32a0ffc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('and this', 'و هذه؟')"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)\n",
    "dataset[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
