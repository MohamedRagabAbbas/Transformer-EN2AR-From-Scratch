{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "55a81d32-1948-46f7-ad43-1a12cbde00a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2fd27eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_seq_len = 128\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9794924",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "199d9264",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostionalEncoding(nn.Module):\n",
    "    def __init__(self,max_seq_len,d_model):\n",
    "        super(PostionalEncoding,self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.encoding = torch.zeros(max_seq_len,d_model)\n",
    "    \n",
    "    def forward(self):\n",
    "        even_index = torch.arange(0,self.d_model,2).float()\n",
    "        domenator = torch.pow(10000,even_index/self.d_model)\n",
    "        position = torch.arange(0,self.max_seq_len).unsqueeze(1)\n",
    "        PE_even = torch.sin(position/domenator)\n",
    "        PE_odd = torch.cos(position/domenator)\n",
    "        stacked = torch.stack([PE_even,PE_odd],dim=2)\n",
    "        PE_flatten = torch.flatten(stacked,start_dim=1,end_dim=2)\n",
    "        return PE_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ec479f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = '<START>'\n",
    "PADDING_TOKEN = '<PADDING>'\n",
    "END_TOKEN = '<END>' \n",
    "\n",
    "## all characters in the arabic language + symbols\n",
    "arabic_voc = [START_TOKEN,'ا','ب','ت','ث','ج','ح','خ','د','ذ','ر','ز','س','ش','ص','ض','ط','ظ','ع','غ','ف','ق','ك','ل','م','ن','ه','و','ي','ء','آ','أ','ؤ','إ','ئ','ئ','ة','ـ','،','؛','؟','٠','١','٢','٣','٤','٥','٦','٧','٨','٩','٪','٫','٬','٭','ٮ','ٯ','ٰ','ٱ','ٲ','ٳ','ٴ','ٵ','ٶ','ٷ','ٸ','ٹ','ٺ','ٻ','ټ','ٽ','پ','ٿ','ڀ','ځ','ڂ','ڃ','ڄ','څ','چ','ڇ','ڈ','ډ','ڊ','ڋ','ڌ','ڍ','ڎ','ڏ','ڐ','ڑ','ڒ','ړ','ڔ','ڕ','ږ','ڗ','ژ','ڙ','ښ','ڛ','ڜ','ڝ','ڞ','ڟ','ڠ','ڡ','ڢ','ڣ','ڤ','ڥ','ڦ','ڧ','ڨ','ک','ڪ','ګ','ڬ','ڭ','ڮ','گ','ڰ','ڱ','ڲ','ڳ','ڴ','ڵ','ڶ','ڷ','ڸ','ڹ','ں','ڻ','ڼ','ڽ','ھ','ڿ','ۀ','ہ','ۂ','ۃ','ۄ','ۅ','ۆ','ۇ','ۈ','ۉ','ۊ','ۋ','ی','ۍ','ێ','ۏ','ې','ۑ','ے','ۓ','۔','ە','ۖ','ۗ','ۘ','ۙ','ۚ','ۛ','ۜ','۝','۞','۟','۠','ۡ','ۢ','ۣ','ۤ','ۥ','ۦ','ۧ',PADDING_TOKEN,END_TOKEN]\n",
    "## all characters in the english language + symbols\n",
    "english_voc = [START_TOKEN, 'A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','0','1','2','3','4','5','6','7','8','9','!','\"','#','$','%','&',\"'\",'(',')','*','+',',','-','.','/',':',';','<','=','>','?','@','[','\\\\',']','^','_','`','{','|','}','~',' ','\\t','\\n','\\r','\\x0b','\\x0c',PADDING_TOKEN,END_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "be4e0089",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_english = {k:v for k,v in enumerate(english_voc)}\n",
    "english_to_index = {v:k for k,v in enumerate(english_voc)}\n",
    "index_to_arabic = {k:v for k,v in enumerate(arabic_voc)}\n",
    "arabic_to_index = {v:k for k,v in enumerate(arabic_voc)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ecefbc",
   "metadata": {},
   "source": [
    "## Sentence Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b80fae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEmbedding(nn.Module):\n",
    "    def __init__(self, language_to_index, max_seq_len, d_model):\n",
    "        super(SentenceEmbedding,self).__init__()\n",
    "        self.vocab_size = len(language_to_index)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.language_to_index = language_to_index\n",
    "        self.position_encoder = PostionalEncoding(max_seq_len,d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.START_TOKEN = START_TOKEN\n",
    "        self.END_TOKEN = END_TOKEN\n",
    "        self.PADDING_TOKEN = PADDING_TOKEN\n",
    "       \n",
    "    def batch_tokenize(self, batch, start_token=True, end_token=True):\n",
    "\n",
    "        def tokenize(sentence, start_token=True, end_token=True):\n",
    "            sentence_word_indicies = [self.language_to_index[token] for token in list(sentence)]\n",
    "            if start_token:\n",
    "                sentence_word_indicies.insert(0, self.language_to_index[self.START_TOKEN])\n",
    "            if end_token:\n",
    "                sentence_word_indicies.append(self.language_to_index[self.END_TOKEN])\n",
    "            for _ in range(len(sentence_word_indicies), self.max_seq_len):\n",
    "                sentence_word_indicies.append(self.language_to_index[self.PADDING_TOKEN])\n",
    "            return torch.tensor(sentence_word_indicies)\n",
    "\n",
    "        tokenized = []\n",
    "        for sentence_num in range(len(batch)):\n",
    "           tokenized.append( tokenize(batch[sentence_num], start_token, end_token) )\n",
    "        \n",
    "        tokenized = torch.stack(tokenized)\n",
    "        return tokenized\n",
    "    \n",
    "    def forward(self, x, start_token= True, end_token=True): \n",
    "        x = self.batch_tokenize(x ,start_token=start_token, end_token=end_token)\n",
    "        x = self.embedding(x)\n",
    "        pos = self.position_encoder()\n",
    "        x = self.dropout(x + pos)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c52f2d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4711,  3.5255,  0.9527,  ...,  0.0000,  0.0000,  0.7886],\n",
      "         [-0.5839,  0.3850,  1.0791,  ...,  2.7149, -1.7831,  1.4967],\n",
      "         [ 1.2782, -1.5748, -0.3536,  ...,  0.2524, -0.5312,  0.0000],\n",
      "         ...,\n",
      "         [-0.0691,  0.0000,  3.5747,  ...,  0.1444,  0.2052,  0.5213],\n",
      "         [ 0.0000,  1.0412,  3.4578,  ...,  0.1444,  0.2053,  0.5213],\n",
      "         [ 1.6961,  0.2505,  0.0000,  ...,  0.1444,  0.2054,  0.5213]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SentenceEmbedding(english_to_index, max_seq_len, d_model)\n",
    "print(tokenizer(['hello world']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
